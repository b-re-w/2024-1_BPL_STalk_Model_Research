{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Llama C++ Python Bindings",
   "id": "f5942b88c4002442"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T16:17:19.854482Z",
     "start_time": "2024-05-25T16:17:19.069756Z"
    }
   },
   "cell_type": "code",
   "source": "from llama_cpp import Llama",
   "id": "257bb141e473bf33",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Pulling models from Hugging Face Hub",
   "id": "8632c9583beecbe7"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-25T16:17:50.496774Z",
     "start_time": "2024-05-25T16:17:22.286018Z"
    }
   },
   "source": [
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"Qwen/Qwen1.5-0.5B-Chat-GGUF\",\n",
    "    filename=\"*q8_0.gguf\",\n",
    "    verbose=False\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qwen1_5-0_5b-chat-q8_0.gguf:   0%|          | 0.00/665M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "646d69355c1f41d1b4a9a07ac29f0693"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IRACK\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\4bitq-5gRLrxl_-py3.11\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\IRACK\\.cache\\huggingface\\hub\\models--Qwen--Qwen1.5-0.5B-Chat-GGUF. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Chat Completion",
   "id": "9bccf85cbf83cff0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T16:18:14.523960Z",
     "start_time": "2024-05-25T16:17:50.497780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm.create_chat_completion(\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an assistant who perfectly describes images.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Describe this image in detail please.\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ],
   "id": "7137e08ed684de1b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-9ab286e5-533d-4c30-b852-8e4fb4f120be',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1716653870,\n",
       " 'model': 'C:\\\\Users\\\\IRACK\\\\.cache\\\\huggingface\\\\hub\\\\models--Qwen--Qwen1.5-0.5B-Chat-GGUF\\\\snapshots\\\\cfab082d2fef4a8736ef384dc764c2fb6887f387\\\\.\\\\qwen1_5-0_5b-chat-q8_0.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \"I'm sorry, but I cannot describe an image without seeing it first. Please provide the image you would like me to describe, and I will do my best to assist you with your request.\"},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 30, 'completion_tokens': 39, 'total_tokens': 69}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### JSON and JSON Schema Mode",
   "id": "ac10a4cf23b0755b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### JSON Mode",
   "id": "96d6941c2b148d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T16:18:41.177835Z",
     "start_time": "2024-05-25T16:18:14.525965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant that outputs in JSON.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020\"},\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_object\",\n",
    "    },\n",
    "    temperature=0.7,\n",
    ")"
   ],
   "id": "3b887e59989fa08b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-08ac2a4b-7ccb-4ab0-a661-a079234fae32',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1716653894,\n",
       " 'model': 'C:\\\\Users\\\\IRACK\\\\.cache\\\\huggingface\\\\hub\\\\models--Qwen--Qwen1.5-0.5B-Chat-GGUF\\\\snapshots\\\\cfab082d2fef4a8736ef384dc764c2fb6887f387\\\\.\\\\qwen1_5-0_5b-chat-q8_0.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '{ \"year\": 2020, \"team\": \"Los Angeles Dodgers\", \"title\": \"World Series\", \"result\": \"Los Angeles Dodgers defeated New York Mets, 6-4\"}'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 35, 'completion_tokens': 42, 'total_tokens': 77}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### JSON Schema Mode",
   "id": "fb980e6aa9bd2af3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T16:18:42.891161Z",
     "start_time": "2024-05-25T16:18:41.178857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant that outputs in JSON.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020\"},\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_object\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"team_name\": {\"type\": \"string\"}},\n",
    "            \"required\": [\"team_name\"],\n",
    "        },\n",
    "    },\n",
    "    temperature=0.7,\n",
    ")"
   ],
   "id": "ab265d63bf2a16f4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-47249210-1b59-4543-b881-310cd6d3bc9b',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1716653921,\n",
       " 'model': 'C:\\\\Users\\\\IRACK\\\\.cache\\\\huggingface\\\\hub\\\\models--Qwen--Qwen1.5-0.5B-Chat-GGUF\\\\snapshots\\\\cfab082d2fef4a8736ef384dc764c2fb6887f387\\\\.\\\\qwen1_5-0_5b-chat-q8_0.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '{ \"team_name\": \"Los Angeles Angels of Baseball\" }'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 35, 'completion_tokens': 13, 'total_tokens': 48}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Function Calling",
   "id": "fed51becc7af6655"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T16:19:25.941822Z",
     "start_time": "2024-05-25T16:19:20.487517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm.create_chat_completion(\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. The assistant calls functions with appropriate input when necessary\"\n",
    "\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Extract Jason is 25 years old\"\n",
    "        }\n",
    "    ],\n",
    "    tools=[{\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"UserDetail\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"title\": \"UserDetail\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\n",
    "                        \"title\": \"Name\",\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"age\": {\n",
    "                        \"title\": \"Age\",\n",
    "                        \"type\": \"integer\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [ \"name\", \"age\" ]\n",
    "            }\n",
    "        }\n",
    "    }],\n",
    "    tool_choice={\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"UserDetail\"\n",
    "        }\n",
    "    }\n",
    ")"
   ],
   "id": "94f041bfdf32bf76",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-1cf03656-5f21-4e0d-ab3a-62dd2f849882',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1716653960,\n",
       " 'model': 'C:\\\\Users\\\\IRACK\\\\.cache\\\\huggingface\\\\hub\\\\models--Qwen--Qwen1.5-0.5B-Chat-GGUF\\\\snapshots\\\\cfab082d2fef4a8736ef384dc764c2fb6887f387\\\\.\\\\qwen1_5-0_5b-chat-q8_0.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': None,\n",
       "    'function_call': {'name': 'UserDetail',\n",
       "     'arguments': '{ \"name\": \"Jason\", \"age\": 25 }'},\n",
       "    'tool_calls': [{'id': 'call__0_UserDetail_cmpl-1cf03656-5f21-4e0d-ab3a-62dd2f849882',\n",
       "      'type': 'function',\n",
       "      'function': {'name': 'UserDetail',\n",
       "       'arguments': '{ \"name\": \"Jason\", \"age\": 25 }'}}]},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'tool_calls'}],\n",
       " 'usage': {'prompt_tokens': 60, 'completion_tokens': 14, 'total_tokens': 74}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Multi-modal Models",
   "id": "47f379d5c0f84337"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T16:19:31.949188Z",
     "start_time": "2024-05-25T16:19:31.946411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_cpp import Llama\n",
    "from llama_cpp.llama_chat_format import MoondreamChatHandler"
   ],
   "id": "c418603ad73d8a73",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T16:21:28.917344Z",
     "start_time": "2024-05-25T16:19:32.484551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_handler = MoondreamChatHandler.from_pretrained(\n",
    "    repo_id=\"vikhyatk/moondream2\",\n",
    "    filename=\"*mmproj*\",\n",
    ")\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"vikhyatk/moondream2\",\n",
    "    filename=\"*text-model*\",\n",
    "    chat_handler=chat_handler,\n",
    "    n_ctx=2048, # n_ctx should be increased to accommodate the image embedding\n",
    ")"
   ],
   "id": "5313f061e742970a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moondream2-mmproj-f16.gguf:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d68fef7a237b40a4a28a8bc73729febf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IRACK\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\4bitq-5gRLrxl_-py3.11\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\IRACK\\.cache\\huggingface\\hub\\models--vikhyatk--moondream2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "moondream2-text-model-f16.gguf:   0%|          | 0.00/2.84G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "943b3cb16d1a4eb3ad2c1bc9c0c60a59"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 245 tensors from C:\\Users\\IRACK\\.cache\\huggingface\\hub\\models--vikhyatk--moondream2\\snapshots\\fa8398d264205ac3890b62e97d3c588268ed9ec4\\.\\moondream2-text-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi2\n",
      "llama_model_loader: - kv   1:                               general.name str              = moondream2\n",
      "llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                           phi2.block_count u32              = 24\n",
      "llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = [\"Ġ t\", \"Ġ a\", \"h e\", \"i n\", \"r e\",...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256\n",
      "llama_model_loader: - type  f32:  147 tensors\n",
      "llama_model_loader: - type  f16:   98 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: mismatch in special tokens definition ( 910/51200 vs 944/51200 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = phi2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 51200\n",
      "llm_load_print_meta: n_merges         = 50000\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_rot            = 32\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 1B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 1.42 B\n",
      "llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) \n",
      "llm_load_print_meta: general.name     = moondream2\n",
      "llm_load_print_meta: BOS token        = 50256 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 50256 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 50256 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 50256 '<|endoftext|>'\n",
      "llm_load_tensors: ggml ctx size =    0.12 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/25 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  2706.27 MiB\n",
      "................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =   384.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.20 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   304.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    12.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 921\n",
      "llama_new_context_with_model: graph splits = 294\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.architecture': 'phi2', 'phi2.context_length': '2048', 'general.name': 'moondream2', 'phi2.attention.head_count_kv': '32', 'phi2.embedding_length': '2048', 'tokenizer.ggml.add_bos_token': 'false', 'phi2.feed_forward_length': '8192', 'tokenizer.ggml.bos_token_id': '50256', 'phi2.block_count': '24', 'phi2.attention.head_count': '32', 'phi2.attention.layer_norm_epsilon': '0.000010', 'phi2.rope.dimension_count': '32', 'tokenizer.ggml.eos_token_id': '50256', 'general.file_type': '1', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.unknown_token_id': '50256'}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T16:22:20.794057Z",
     "start_time": "2024-05-25T16:21:47.882905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = llm.create_chat_completion(\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\" : \"text\", \"text\": \"What's in this image?\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\" } }\n",
    "\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")"
   ],
   "id": "6dbb54b4ddfb29ff",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   47680.51 ms\n",
      "llama_print_timings:      sample time =      16.83 ms /    76 runs   (    0.22 ms per token,  4516.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (-nan(ind) ms per token, -nan(ind) tokens per second)\n",
      "llama_print_timings:        eval time =    5397.93 ms /    76 runs   (   71.03 ms per token,    14.08 tokens per second)\n",
      "llama_print_timings:       total time =    5518.26 ms /    76 tokens\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T16:22:56.823345Z",
     "start_time": "2024-05-25T16:22:56.820581Z"
    }
   },
   "cell_type": "code",
   "source": "print(response[\"choices\"][0][\"message\"][\"content\"])",
   "id": "1ae3fed19faa85d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The image depicts a long wooden walkway that stretches across the center of the frame, leading towards an open field. The path is surrounded by lush green grass and trees, creating a serene atmosphere. The sky above is filled with clouds, suggesting a partly cloudy day. The scene conveys a sense of tranquility as one would expect to find in such a peaceful setting.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5ba25d07b1b0e6be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
