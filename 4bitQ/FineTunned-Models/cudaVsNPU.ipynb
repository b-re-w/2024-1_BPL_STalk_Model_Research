{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-23T03:00:24.844932Z",
     "start_time": "2024-05-23T03:00:20.532236Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, TextStreamer, AutoModelForCausalLM\n",
    "import intel_npu_acceleration_library\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T03:00:29.579925Z",
     "start_time": "2024-05-23T03:00:25.610974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, use_cache=True).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_default_system_prompt=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "streamer = TextStreamer(tokenizer, skip_special_tokens=True)"
   ],
   "id": "6b0e92408f966b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T02:46:02.388005Z",
     "start_time": "2024-05-23T02:45:54.466055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Compile model for the NPU\")\n",
    "model = intel_npu_acceleration_library.compile(model, dtype=torch.int8)"
   ],
   "id": "e9b5891240641dd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile model for the NPU\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T02:50:57.535637Z",
     "start_time": "2024-05-23T02:46:06.170845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = input(\"Ask something: \")\n",
    "prefix = tokenizer(query, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "\n",
    "generation_kwargs = dict(\n",
    "    input_ids=prefix,\n",
    "    streamer=streamer,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "print(\"Run inference\")\n",
    "_ = model.generate(**generation_kwargs)"
   ],
   "id": "1b5f12ced4312d10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run inference\n",
      "ì‚¬ìì„±ì–´ í•˜ë‚˜ë¥¼ ì¶”ì²œí•´ì¤˜ ì•¼ë§ë¡œ ì–¼ë¥¸ì— ê°€ ê°™ì´ë‚˜ ì„œë¡œ ì„œë¡œ ë„ë¦¬ë¼ í•˜ì\n",
      "\n",
      "ë¶ë§ˆí¬ 5ë²ˆ\n",
      "\n",
      "ì œ ì•Œì—†ëŠ” ì‚¬ëŒì€ ê·¸ëŸ° ì¼ì„ í•˜ëŠ” ì Šì€ ìì™€ ê°™ì€ ì œ ë§ì„ ë„ˆí¬ë¥¼ ë•ì…¨ê±°ë‹ˆ ë‹¤ë…€ì™€ì„œ ë†€ì•˜ëŠ”ë°ìš” ì‚¬ë‘ì˜ ë•…ì„ ë³´ì•˜ìœ¼ë‹ˆê¹Œ? ì˜†ì— ì˜ˆì˜ê²Œ ì˜ ë˜ì™€ ê·¸ê²ƒì´ ì˜ˆì¨ì„ ì¶œì‹œí•  ê²ƒì…ë‹ˆë‹¤ ë†€ì•˜ìŠµë‹ˆê¹Œ ì œê°€ ê·¸ê²ƒì— ì• í–‰í•  ê²ƒì€ ì˜†ìœ¼ë¡œ ê°€ë¦¬ì§€ ë§ë¼ ë³´ì‹œëŠ” ë²•ìœ¼ë¡œ ì„œë¡œ ì„œë¡œ ë¨¼ì§€ ê°€ì§€ê³  ê°ˆì•˜ìŠµë‹ˆë‹¤ ì˜ˆì•½ì´ ìˆìœ¼ì‹œëƒ„ì—ì„œ ë§¹ìˆ ì„ ë§ˆì‹œì…¨ê³  ì˜¤ë¦¬ëŠ” ë¶„ì—ê²Œ ë°ë¦¬ë¼ ì˜†ìœ¼ë¡œ ê°€ì§€ê³  ê°ˆ ìˆ˜ ìˆì—ˆìœ¼ë‹ˆê¹Œ í•˜ë£¡ ë³´ë‹ˆ ì•„ë²„ì§€ ë‹¬ë¦¬ë ¤ì™€ì„œ í˜¹ë°›ì•„ë“¤ì•„ì£¼ì…¨ê³  ì˜ˆì»¨ëŒ€ ê±´ë“œë¼ ì£¼ì‹­ë‹ˆê¹Œ ì˜¤ëŠ˜ ì˜¨ ì¼ì„ í•  ìˆ˜ ìˆì—ˆë…¸ë‹ˆê¹Œ ì‚¬ëŒë“¤ì´ ë¨¹ê³  ë§ˆ\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T02:52:02.194565Z",
     "start_time": "2024-05-23T02:52:02.180085Z"
    }
   },
   "cell_type": "code",
   "source": "del model",
   "id": "4729e7db0d554e8a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T03:01:02.246596Z",
     "start_time": "2024-05-23T03:01:00.841046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Move model to GPU\")\n",
    "model.to(device=\"cuda\", non_blocking=True)"
   ],
   "id": "5c6d943394acbe28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Move model to GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T03:01:19.611248Z",
     "start_time": "2024-05-23T03:01:05.007633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = input(\"Ask something: \")\n",
    "prefix = tokenizer(query, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "\n",
    "generation_kwargs = dict(\n",
    "    input_ids=prefix.to(device=\"cuda\", non_blocking=True),\n",
    "    streamer=streamer,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "print(\"Run inference\")\n",
    "_ = model.generate(**generation_kwargs)"
   ],
   "id": "10758eacdc4d3ffb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run inference\n",
      "ì‚¬ìì„±ì–´ í•˜ë‚˜ë¥¼ "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python311\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¶”ì²œí•´ì¤˜ìš”. ì¢‹ì•„ìš”! í–‰ë³µí•˜ì„¸ìš”! ë„ˆë¬´ ê·€ì°®ì•„ìš” ì´ë ‡ê²Œ ì“°ê²Œ í•´ì„œ ì¢‹ì•„ìš” í•˜ì§€ë§Œ ê·¸ëŸ¼ì¤„ê±°ì•¼ ë‚  ì“¸ê²Œìš” ì˜ ì“°ì„¸ìš” ğŸ˜ŠğŸ¤— ì°¸ í˜ë“  ê²ƒ ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "aaed5fb8c82b8f1a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
