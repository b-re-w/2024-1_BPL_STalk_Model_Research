{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-23T03:00:24.844932Z",
     "start_time": "2024-05-23T03:00:20.532236Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, TextStreamer, AutoModelForCausalLM\n",
    "import intel_npu_acceleration_library\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T03:00:29.579925Z",
     "start_time": "2024-05-23T03:00:25.610974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, use_cache=True).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_default_system_prompt=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "streamer = TextStreamer(tokenizer, skip_special_tokens=True)"
   ],
   "id": "6b0e92408f966b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T02:46:02.388005Z",
     "start_time": "2024-05-23T02:45:54.466055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Compile model for the NPU\")\n",
    "model = intel_npu_acceleration_library.compile(model, dtype=torch.int8)"
   ],
   "id": "e9b5891240641dd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile model for the NPU\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T02:50:57.535637Z",
     "start_time": "2024-05-23T02:46:06.170845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = input(\"Ask something: \")\n",
    "prefix = tokenizer(query, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "\n",
    "generation_kwargs = dict(\n",
    "    input_ids=prefix,\n",
    "    streamer=streamer,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "print(\"Run inference\")\n",
    "_ = model.generate(**generation_kwargs)"
   ],
   "id": "1b5f12ced4312d10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run inference\n",
      "사자성어 하나를 추천해줘 야말로 얼른에 가 같이나 서로 서로 도리라 하자\n",
      "\n",
      "북마크 5번\n",
      "\n",
      "제 알없는 사람은 그런 일을 하는 젊은 자와 같은 제 말을 너희를 돕셨거니 다녀와서 놀았는데요 사랑의 땅을 보았으니까? 옆에 예쁘게 잘 되와 그것이 예쁨을 출시할 것입니다 놀았습니까 제가 그것에 애행할 것은 옆으로 가리지 말라 보시는 법으로 서로 서로 먼지 가지고 갈았습니다 예약이 있으시냄에서 맹술을 마시셨고 오리는 분에게 데리라 옆으로 가지고 갈 수 있었으니까 하룡 보니 아버지 달리려와서 혹받아들아주셨고 예컨대 건드라 주십니까 오늘 온 일을 할 수 있었노니까 사람들이 먹고 마\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T02:52:02.194565Z",
     "start_time": "2024-05-23T02:52:02.180085Z"
    }
   },
   "cell_type": "code",
   "source": "del model",
   "id": "4729e7db0d554e8a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T03:01:02.246596Z",
     "start_time": "2024-05-23T03:01:00.841046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Move model to GPU\")\n",
    "model.to(device=\"cuda\", non_blocking=True)"
   ],
   "id": "5c6d943394acbe28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Move model to GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T03:01:19.611248Z",
     "start_time": "2024-05-23T03:01:05.007633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = input(\"Ask something: \")\n",
    "prefix = tokenizer(query, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "\n",
    "generation_kwargs = dict(\n",
    "    input_ids=prefix.to(device=\"cuda\", non_blocking=True),\n",
    "    streamer=streamer,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "print(\"Run inference\")\n",
    "_ = model.generate(**generation_kwargs)"
   ],
   "id": "10758eacdc4d3ffb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run inference\n",
      "사자성어 하나를 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python311\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추천해줘요. 좋아요! 행복하세요! 너무 귀찮아요 이렇게 쓰게 해서 좋아요 하지만 그럼줄거야 날 쓸게요 잘 쓰세요 😊🤗 참 힘든 것 입니다.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "aaed5fb8c82b8f1a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
