{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {
    "id": "QOdqtsFScyGI"
   },
   "cell_type": "markdown",
   "source": [
    "# Voice Model\n",
    "\n",
    "References:\n",
    "\n",
    "[Speaker Recognition - pyannote-audio](\"https://github.com/pyannote/pyannote-audio\")\n",
    "\n",
    "[Building a Speaker Identification System](\"https://medium.com/analytics-vidhya/building-a-speaker-identification-system-from-scratch-with-deep-learning-f4c4aa558a56\")\n",
    "\n",
    "[상담사 통화녹음 화자분리](\"https://youngseo-computerblog.tistory.com/120\")\n",
    "\n",
    "_\n",
    "\n",
    "Implementations:\n",
    "\n",
    "[Faster Whisper](\"https://github.com/systran/faster-whisper\")\n",
    "\n",
    "[Audio Embedding - wespeaker-voxceleb-resnet34-LM](\"https://huggingface.co/pyannote/wespeaker-voxceleb-resnet34-LM\") | [resnet293-LM](\"https://huggingface.co/Wespeaker/wespeaker-voxceleb-resnet293-LM/tree/main\")\n",
    "\n",
    "[whisper_streaming](\"https://github.com/ufal/whisper_streaming\")\n",
    "\n",
    "[whisper live](\"https://github.com/collabora/WhisperLive\")"
   ]
  },
  {
   "metadata": {
    "id": "hML1D0dOcyGJ"
   },
   "cell_type": "markdown",
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "metadata": {
    "id": "xTBlPtnzcyGK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0c9f2131-4929-48d1-9bcf-812f4c8bd1cc"
   },
   "cell_type": "code",
   "source": [
    "!pip install faster-whisper\n",
    "!pip install pyannote-audio"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "Qow5G5O9cyGL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d0a7c933-d2e7-4541-9ba9-a6f7f5cac831",
    "ExecuteTime": {
     "end_time": "2024-05-30T15:44:36.428263Z",
     "start_time": "2024-05-30T15:44:28.291023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pyannote.audio import Audio\n",
    "from pyannote.core import Segment\n",
    "\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"TorchAudio:\", torchaudio.__version__)\n",
    "print(\"Uses Device:\", device.upper())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.3.0+cu121\n",
      "TorchAudio: 2.3.0+cu121\n",
      "Uses Device: CUDA\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Whisper"
  },
  {
   "cell_type": "code",
   "source": [
    "# delete model if low on GPU resources\n",
    "import gc\n",
    "try:\n",
    "    del model\n",
    "    print(\"Model Deleted.\")\n",
    "except NameError as e:\n",
    "    print(e)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "id": "IuT5Kg2weSDS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ec3be78b-ccb0-46eb-9383-610d090243ab",
    "ExecuteTime": {
     "end_time": "2024-05-30T15:38:41.394347Z",
     "start_time": "2024-05-30T15:38:41.279310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'model' is not defined\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "id": "QPq3qbmocyGL",
    "ExecuteTime": {
     "end_time": "2024-05-30T15:44:36.432665Z",
     "start_time": "2024-05-30T15:44:36.429274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "language = None\n",
    "model_size = \"medium\"  #@param ['tiny', 'base', 'small', 'medium', 'large', 'large-v2', 'large-v3']\n",
    "compute_type = \"int8\"  #@param ['float16', 'int8']"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "id": "pMFTOYG5cyGM",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-05-30T15:44:38.249020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run on GPU with selected compute type\n",
    "model = WhisperModel(model_size, device=device, compute_type=compute_type)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "FS--A0SxcyGM",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 94
    },
    "outputId": "31f82bec-ea34-4fec-e767-9020a45627c5",
    "ExecuteTime": {
     "end_time": "2024-05-30T15:44:45.696896Z",
     "start_time": "2024-05-30T15:44:45.686771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# upload audio file\n",
    "try:\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    audio_path = next(iter(uploaded))\n",
    "except ModuleNotFoundError:\n",
    "    audio_path = \"./sample_conversation/kor/conversation_kor_2_1.wav\"\n",
    "    from os.path import isfile\n",
    "    assert isfile(audio_path)\n",
    "\n",
    "if audio_path[-3:] != \"wav\":\n",
    "    import subprocess\n",
    "    subprocess.call([\"ffmpeg\", \"-i\", audio_path, \"audio.wav\", \"-y\"])\n",
    "    audio_path = \"audio.wav\"\n",
    "audio_path"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./sample_conversation/kor/conversation_kor_2_1.wav'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "id": "rGX6uSIIcyGM",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "outputId": "736aaa51-ff64-4f93-b0ec-4b8305022c26"
   },
   "cell_type": "code",
   "source": [
    "from IPython.display import Audio as AudioDisplay\n",
    "audio = Audio()\n",
    "waveform, sample_rate = audio(audio_path)\n",
    "AudioDisplay(waveform, rate=sample_rate)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "Og3T8f5OcyGM"
   },
   "cell_type": "code",
   "source": [
    "# Transcribe\n",
    "segments, info = model.transcribe(audio_path, beam_size=5, language=language, word_timestamps=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "GCAKQRhlcyGM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9621180b-e13c-4c89-eb20-103efdf0b567"
   },
   "cell_type": "code",
   "source": [
    "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "for segment in segments:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Speaker Diarization"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://raw.githubusercontent.com/b-re-w/2024-1_BPL_STalk_Model_Research/main/SpeakerDiarization/res/1_V6kstNiDGG3knzsZ-DcFyw.jpg\"/>"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://raw.githubusercontent.com/b-re-w/2024-1_BPL_STalk_Model_Research/main/SpeakerDiarization/res/1_cGMVhv0dNZTM6gPua4uzAA.jpg\"/>"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://raw.githubusercontent.com/b-re-w/2024-1_BPL_STalk_Model_Research/main/SpeakerDiarization/res/1_yzq0c8tEruvTEf1UlVezSA.jpg\"/>"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# upload audio file\n",
    "try:\n",
    "    from google.colab import files\n",
    "    uploaded = iter(files.upload())\n",
    "    speaker1 = next(uploaded)\n",
    "    speaker2 = next(uploaded)\n",
    "except ModuleNotFoundError:\n",
    "    speaker1 = \"./sample_conversation/kor/kor_2_1.wav\"\n",
    "    speaker2 = \"./sample_conversation/kor/kor_2_2.wav\"\n",
    "    from os.path import isfile\n",
    "    assert isfile(speaker1) and isfile(speaker2)\n",
    "\n",
    "if audio_path[-3:] != \"wav\":\n",
    "    import subprocess\n",
    "    subprocess.call([\"ffmpeg\", \"-i\", speaker1, \"speaker1.wav\", \"-y\"])\n",
    "    subprocess.call([\"ffmpeg\", \"-i\", speaker1, \"speaker2.wav\", \"-y\"])\n",
    "    speaker1 = \"speaker1.wav\"\n",
    "    speaker2 = \"speaker2.wav\"\n",
    "speaker1, speaker2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import Audio as AudioDisplay\n",
    "audio = Audio()\n",
    "waveform, sample_rate = audio(speaker1)\n",
    "AudioDisplay(waveform, rate=sample_rate)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import Audio as AudioDisplay\n",
    "audio = Audio()\n",
    "waveform, sample_rate = audio(speaker2)\n",
    "AudioDisplay(waveform, rate=sample_rate)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Using ResNet - 293"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install git+https://github.com/wenet-e2e/wespeaker.git",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# instantiate pretrained model\n",
    "from huggingface_hub import hf_hub_download\n",
    "import wespeaker\n",
    "\n",
    "model_list = [\n",
    "    \"Wespeaker/wespeaker-voxceleb-resnet34-LM\",\n",
    "    \"Wespeaker/wespeaker-voxceleb-resnet152-LM\",\n",
    "    \"Wespeaker/wespeaker-voxceleb-resnet221-LM\",\n",
    "    \"Wespeaker/wespeaker-voxceleb-resnet293-LM\",\n",
    "    \"Wespeaker/wespeaker-ecapa-tdnn512-LM\"\n",
    "]\n",
    "\n",
    "model_id = model_list[3]\n",
    "\n",
    "model_binary = model_id.replace(\"Wespeaker/wespeaker-\", \"\").replace(\"-\", \"_\")+\".onnx\"\n",
    "root_dir = hf_hub_download(model_id, filename=model_binary).replace(model_binary, \"\")\n",
    "hf_hub_download(model_id, filename=\"avg_model.pt\")\n",
    "hf_hub_download(model_id, filename=\"config.yaml\")\n",
    "resnet = wespeaker.load_model_local(root_dir)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "resnet.register('민서', speaker1)\n",
    "resnet.register('연우', speaker2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_embedding(self, pcm, sample_rate):\n",
    "    if self.apply_vad:\n",
    "        # TODO(Binbin Zhang): Refine the segments logic, here we just\n",
    "        # suppose there is only silence at the start/end of the speech\n",
    "        segments = self.vad.get_speech_timestamps(audio_path, return_seconds=True)\n",
    "        pcmTotal = torch.Tensor()\n",
    "        if len(segments) > 0:  # remove all the silence\n",
    "            for segment in segments:\n",
    "                start = int(segment['start'] * sample_rate)\n",
    "                end = int(segment['end'] * sample_rate)\n",
    "                pcmTemp = pcm[0, start:end]\n",
    "                pcmTotal = torch.cat([pcmTotal, pcmTemp], 0)\n",
    "            pcm = pcmTotal.unsqueeze(0)\n",
    "        else:  # all silence, nospeech\n",
    "            return None\n",
    "    pcm = pcm.to(torch.float)\n",
    "    if sample_rate != self.resample_rate:\n",
    "        pcm = torchaudio.transforms.Resample(\n",
    "            orig_freq=sample_rate, new_freq=self.resample_rate)(pcm)\n",
    "    feats = self.compute_fbank(\n",
    "        pcm,\n",
    "        sample_rate=self.resample_rate,\n",
    "        cmn=True\n",
    "    )\n",
    "    feats = feats.unsqueeze(0)\n",
    "    feats = feats.to(self.device)\n",
    "    self.model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = self.model(feats)\n",
    "        outputs = outputs[-1] if isinstance(outputs, tuple) else outputs\n",
    "    embedding = outputs[0].to(torch.device('cpu'))\n",
    "    return embedding"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def recognize(self, pcm, sample_rate):\n",
    "    q = extract_embedding(self, pcm, sample_rate)\n",
    "    best_score = 0.0\n",
    "    best_name = ''\n",
    "    for name, e in self.table.items():\n",
    "        score = self.cosine_similarity(q, e)\n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            best_name = name\n",
    "    result = {'name': best_name, 'confidence': best_score}\n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "segments, info = model.transcribe(audio_path, beam_size=5, language=language, word_timestamps=False)\n",
    "for segment in segments:\n",
    "    embedding = audio.crop(audio_path, Segment(segment.start, segment.end))\n",
    "    speaker = recognize(resnet, *embedding)\n",
    "    print(\"[%s] [%.2fs -> %.2fs] %s\" % (speaker['name'], segment.start, segment.end, segment.text))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualizing Speaker Diarization"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Your existing code for clustering and labeling segments...\n",
    "\n",
    "# Perform PCA to reduce the dimensionality of embeddings to 2D\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, segment in enumerate(segments):\n",
    "    speaker_id = labels[i] + 1\n",
    "    x, y = embeddings_2d[i]\n",
    "    plt.scatter(x, y, label=f'SPEAKER {speaker_id}')\n",
    "\n",
    "plt.title(\"Speaker Diarization Clusters (PCA Visualization)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Using ResNet - 34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from scipy.spatial.distance import cdist",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# instantiate pretrained model\n",
    "from pyannote.audio import Model\n",
    "resnet = Model.from_pretrained(\"pyannote/wespeaker-voxceleb-resnet34-LM\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyannote.audio import Inference\n",
    "inference = Inference(resnet, window=\"whole\")\n",
    "inference.to(torch.device(device))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embedding1 = np.expand_dims(inference(speaker1), axis=0)\n",
    "embedding2 = np.expand_dims(inference(speaker2), axis=0)\n",
    "print(embedding1.shape, embedding2.shape)\n",
    "# `embeddingX` is (1 x D) numpy array extracted from the file as a whole."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "segments, info = model.transcribe(audio_path, beam_size=5, language=language, word_timestamps=False)\n",
    "for segment in segments:\n",
    "    embedding = inference.crop(audio_path, Segment(segment.start, segment.end))\n",
    "    embedding = np.expand_dims(embedding, axis=0)\n",
    "    distance1 = cdist(embedding, embedding1, metric=\"cosine\")[0, 0]\n",
    "    distance2 = cdist(embedding, embedding2, metric=\"cosine\")[0, 0]\n",
    "    speaker = \"SPEAKER 1\" if distance1 < distance2 else \"SPEAKER 2\"\n",
    "    print(\"[%s] [%.2fs -> %.2fs] %s\" % (speaker, segment.start, segment.end, segment.text))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Using SpeechBrain Model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# If you're going to use SpeechBrain model\n",
    "!pip install git+https://github.com/speechbrain/speechbrain.git@65c0113"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
    "\n",
    "sb_model = PretrainedSpeakerEmbedding(\n",
    "    embedding=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "    device=torch.device(\"cuda\")\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# convert audio to mono from\n",
    "_, sample_rate = audio(speaker1)\n",
    "mono = Audio(sample_rate, mono=\"downmix\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# convert each speaker audio to mono\n",
    "embedding1 = sb_model(mono(speaker1)[0][None])\n",
    "embedding2 = sb_model(mono(speaker2)[0][None])\n",
    "print(embedding1.shape, embedding2.shape)\n",
    "# `embeddingX` is (1 x D) numpy array extracted from the file as a whole."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# calculate cosine distance between two embeddings just for testing\n",
    "distance = cdist(embedding1, embedding2, metric=\"cosine\")[0,0]\n",
    "distance\n",
    "# `distance` is a `float` describing how dissimilar speakers 1 and 2 are."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "segments, info = model.transcribe(audio_path, beam_size=5, language=language, word_timestamps=False)\n",
    "for segment in segments:\n",
    "    sound = mono.crop(audio_path, Segment(segment.start, segment.end))\n",
    "    embedding = sb_model(sound[0][None])\n",
    "    distance1 = cdist(embedding, embedding1, metric=\"cosine\")[0, 0]\n",
    "    distance2 = cdist(embedding, embedding2, metric=\"cosine\")[0, 0]\n",
    "    speaker = \"SPEAKER 1\" if distance1 < distance2 else \"SPEAKER 2\"\n",
    "    print(\"[%s] [%.2fs -> %.2fs] %s\" % (speaker, segment.start, segment.end, segment.text))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Miscellaneous"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import wave\n",
    "import contextlib"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with contextlib.closing(wave.open(path, 'r')) as f:\n",
    "    frames = f.getnframes()\n",
    "    rate = f.getframerate()\n",
    "    duration = frames / float(rate)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from whisper_streaming.whisper_online import FasterWhisperASR, OnlineASRProcessor\n",
    "\n",
    "src_lan = \"en\"  # source language\n",
    "#tgt_lan = \"en\"  # target language  -- same as source for ASR, \"en\" if translate task is used\n",
    "\n",
    "asr = FasterWhisperASR(src_lan, \"large-v2\")  # loads and wraps Whisper model\n",
    "# set options:\n",
    "# asr.set_translate_task()  # it will translate from lan into English\n",
    "# asr.use_vad()  # set using VAD"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "online = OnlineASRProcessor(asr)  # create processing object with default buffer trimming option\n",
    "\n",
    "audio_has_not_ended = True\n",
    "\n",
    "while audio_has_not_ended:   # processing loop:\n",
    "    a = audio.crop(path, Segment(0.0, 0.2))  # receive new audio chunk (and e.g. wait for min_chunk_size seconds first, ...)\n",
    "    online.insert_audio_chunk(a)\n",
    "    o = online.process_iter()\n",
    "    print(o)  # do something with current partial output\n",
    "# at the end of this audio processing\n",
    "o = online.finish()\n",
    "print(o)   # do something with the last output\n",
    "\n",
    "\n",
    "online.init()  # refresh if you're going to re-use the object for the next audio"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}
