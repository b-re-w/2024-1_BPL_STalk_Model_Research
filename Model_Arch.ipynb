{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# STalk Model Architecture",
   "id": "7735c9f892bfde57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preparation",
   "id": "8dd61067cd95c518"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "USE_CUDA = False",
   "id": "9504df697eb20002",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install huggingface_hub transformers",
   "id": "cf224e2df50dc3ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install intel-npu-acceleration-library",
   "id": "16c6a20ce68f6b47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if USE_CUDA:\n",
    "    !pip uninstall torch torchvision torchaudio -y\n",
    "    !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "else:\n",
    "    !pip uninstall torch torchvision torchaudio -y\n",
    "    !pip install torch torchvision torchaudio"
   ],
   "id": "78b5e20d2e4794ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "if USE_CUDA:\n",
    "    !pip uninstall llama-cpp-python -y\n",
    "    !pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121\n",
    "else:\n",
    "    !pip uninstall llama-cpp-python -y\n",
    "    !pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install pyannote-audio",
   "id": "c8a543d6de42e52a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install faster-whisper",
   "id": "5d9fa892f0cca9f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install git+https://github.com/wenet-e2e/wespeaker.git",
   "id": "e98497e9df4c9237",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install pyaudio",
   "id": "aee2deb6480d288b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize Pretrained Model",
   "id": "52b6bde35783ad11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:31:44.248895Z",
     "start_time": "2024-06-09T11:31:33.176284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import intel_npu_acceleration_library\n",
    "\n",
    "import gc\n",
    "\n",
    "import wave\n",
    "import pyaudio\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "from pyannote.audio import Audio\n",
    "from pyannote.core import Segment\n",
    "\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "import wespeaker\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "if not torch.cuda.is_available():\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"INFO: CUDA is diabled on this machine.\\n\\n\")\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"TorchAudio:\", torchaudio.__version__)\n",
    "print(\"Uses Device:\", DEVICE.upper())"
   ],
   "id": "5ac9644848ee0e4c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: CUDA is diabled on this machine.\n",
      "\n",
      "\n",
      "PyTorch: 2.3.1+cpu\n",
      "TorchAudio: 2.3.1+cpu\n",
      "Uses Device: CPU\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:31:44.255202Z",
     "start_time": "2024-06-09T11:31:44.249901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ChatHistory(list):\n",
    "    messages = []\n",
    "    \n",
    "    @classmethod\n",
    "    def add_messages(cls, role, content):\n",
    "        if isinstance(content, str):\n",
    "            cls.messages.append({ 'role': role, 'content': content })\n",
    "        else:\n",
    "            for r, c in zip(role, content):\n",
    "                cls.messages.append({ 'role': r, 'content': c })\n",
    "    \n",
    "    @classmethod\n",
    "    def create_prompt(cls, system_prompt: str, user_prompt: str = \"\"):\n",
    "        return [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            *cls.messages,\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt\n",
    "            }\n",
    "        ]"
   ],
   "id": "af46f0c60c545b42",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:31:44.271160Z",
     "start_time": "2024-06-09T11:31:44.256209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def token_stream(token):\n",
    "    delta = token[\"choices\"][0][\"delta\"]\n",
    "    if \"content\" not in delta:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return delta[\"content\"]"
   ],
   "id": "4d3d90098a358928",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:31:44.277727Z",
     "start_time": "2024-06-09T11:31:44.271684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_llama3():\n",
    "    model_id = \"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\"\n",
    "\n",
    "    chat = Llama.from_pretrained(\n",
    "        repo_id=model_id,\n",
    "        filename=\"*Q4_K_M.gguf\",\n",
    "        #chat_format=\"llama-3\",\n",
    "        verbose=False\n",
    "    ).create_chat_completion\n",
    "    \n",
    "    def llama3(system_prompt, user_prompt, temp=0.5, show_prompt=False):\n",
    "        prompt = ChatHistory.create_prompt(system_prompt, user_prompt)\n",
    "\n",
    "        if show_prompt:\n",
    "            print(\"PROMPT:\")\n",
    "            for line in prompt:\n",
    "                print(line)\n",
    "            print()\n",
    "        \n",
    "        return chat(prompt, temperature=temp, stream=True)\n",
    "    \n",
    "    return llama3"
   ],
   "id": "dcc08e168c69e1b0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:31:44.284045Z",
     "start_time": "2024-06-09T11:31:44.278733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_whisper():\n",
    "    model_size = \"medium\"  #@param ['tiny', 'base', 'small', 'medium', 'large', 'large-v2', 'large-v3']\n",
    "    compute_type = \"int8\"  #@param ['float16', 'int8']\n",
    "\n",
    "    return WhisperModel(model_size, device=DEVICE, compute_type=compute_type).transcribe"
   ],
   "id": "b89792661b6ec828",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:31:44.290819Z",
     "start_time": "2024-06-09T11:31:44.285648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_embedding(model, pcm, sample_rate):\n",
    "    pcm = pcm.to(torch.float)\n",
    "    if sample_rate != model.resample_rate:\n",
    "        pcm = torchaudio.transforms.Resample(\n",
    "            orig_freq=sample_rate, new_freq=model.resample_rate)(pcm)\n",
    "    feats = model.compute_fbank(\n",
    "        pcm,\n",
    "        sample_rate=model.resample_rate,\n",
    "        cmn=True\n",
    "    )\n",
    "    feats = feats.unsqueeze(0)\n",
    "    feats = feats.to(model.device)\n",
    "    model.model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.model(feats)\n",
    "        outputs = outputs[-1] if isinstance(outputs, tuple) else outputs\n",
    "    embedding = outputs[0].to(torch.device('cpu'))\n",
    "    return embedding"
   ],
   "id": "30d64a4f6687bd63",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:31:44.296100Z",
     "start_time": "2024-06-09T11:31:44.291825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def recognize(model, pcm, sample_rate):\n",
    "    q = extract_embedding(model, pcm, sample_rate)\n",
    "    best_score = 0.0\n",
    "    best_name = ''\n",
    "    for name, e in model.table.items():\n",
    "        score = model.cosine_similarity(q, e)\n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            best_name = name\n",
    "        del score\n",
    "        gc.collect()\n",
    "    return {'name': best_name, 'confidence': best_score}"
   ],
   "id": "afa012c69b8b6bfb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:31:44.302626Z",
     "start_time": "2024-06-09T11:31:44.297106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_resnet152():\n",
    "    model_id = \"Wespeaker/wespeaker-voxceleb-resnet152-LM\"\n",
    "    model_name = model_id.replace(\"Wespeaker/wespeaker-\", \"\").replace(\"-\", \"_\")\n",
    "    \n",
    "    root_dir = hf_hub_download(model_id, filename=model_name+\".onnx\").replace(model_name+\".onnx\", \"\")\n",
    "    \n",
    "    import os\n",
    "    if not os.path.isfile(root_dir+\"avg_model.pt\"):\n",
    "        os.rename(hf_hub_download(model_id, filename=model_name+\".pt\"), root_dir+\"avg_model.pt\")\n",
    "    if not os.path.isfile(root_dir+\"config.yaml\"):\n",
    "        os.rename(hf_hub_download(model_id, filename=model_name+\".yaml\"), root_dir+\"config.yaml\")\n",
    "\n",
    "    resnet = wespeaker.load_model_local(root_dir)\n",
    "\n",
    "    #print(\"Compile model for the NPU\")\n",
    "    #resnet.model = intel_npu_acceleration_library.compile(resnet.model)\n",
    "    \n",
    "    def resnet152(ado, sample_rate=None):\n",
    "        if isinstance(ado, str):\n",
    "            return resnet.recognize(ado)\n",
    "        else:\n",
    "            return recognize(resnet, ado, sample_rate)\n",
    "    \n",
    "    resnet152.__dict__['register'] = lambda *args, **kwargs: resnet.register(*args, **kwargs)\n",
    "    \n",
    "    return resnet152"
   ],
   "id": "749f1c87edfcaf13",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:31:46.260362Z",
     "start_time": "2024-06-09T11:31:44.303630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llama3 = get_llama3()\n",
    "print(\"INFO: Llama3 Ready -\", llama3)"
   ],
   "id": "84afaa73fc994074",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Llama3 Ready - <function get_llama3.<locals>.llama3 at 0x000001828EBAA3E0>\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:31:49.660903Z",
     "start_time": "2024-06-09T11:31:46.261368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "whisper = get_whisper()\n",
    "print(\"INFO: Whisper Ready -\", whisper)"
   ],
   "id": "60e076b7f0b0d8d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Whisper Ready - <bound method WhisperModel.transcribe of <faster_whisper.transcribe.WhisperModel object at 0x000001828EB17B10>>\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:31:50.123267Z",
     "start_time": "2024-06-09T11:31:49.661911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "audio = Audio()\n",
    "resnet152 = get_resnet152()\n",
    "print(\"INFO: ResNet152 Ready -\", resnet152)"
   ],
   "id": "37aad593f186941f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:unexpected tensor: projection.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: ResNet152 Ready - <function get_resnet152.<locals>.resnet152 at 0x000001828EBAB060>\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Ready",
   "id": "7822a1e86b0b13cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Insert System Chat Template to Llama3",
   "id": "aed1f7b006c23d26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:31:50.127645Z",
     "start_time": "2024-06-09T11:31:50.124272Z"
    }
   },
   "cell_type": "code",
   "source": "system_prompt = \"You are a helpful, smart, kind, and efficient Conversation Analysis and Recommendation AI System. You always fulfill the user's requests to the best of your ability. You need to keep listen to the conversations. Please answer in Korean language.\"",
   "id": "57604fee7594a215",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:32:01.140194Z",
     "start_time": "2024-06-09T11:31:50.130159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for chunk in llama3(system_prompt, \"\"):\n",
    "    print(token_stream(chunk), end=\"\", flush=True)\n",
    "print()"
   ],
   "id": "3440563cb7b1e8a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😊\n",
      "\n",
      "안녕하세요! 저는 지능형 대화 분석 및 추천 AI 시스템입니다. 사용자의 요청을 최선을 다해 충족합니다. 대화를.listen하고 답변을 제공할 것입니다. 한국어로 답변이 가능합니다. 무엇을 도와드릴까요? 🤔\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Speaker Registration to ResNet293",
   "id": "394a61aaef06b005"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:32:01.149519Z",
     "start_time": "2024-06-09T11:32:01.143199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "speaker1 = \"민서\", \"./SpeakerDiarization/sample_conversation/real/sentence_F.wav\"\n",
    "speaker2 = \"연우\", \"./SpeakerDiarization/sample_conversation/real/sentence_M.wav\"\n",
    "speaker1, speaker2"
   ],
   "id": "d7d65905c9926461",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('민서', './SpeakerDiarization/sample_conversation/real/sentence_F.wav'),\n",
       " ('연우', './SpeakerDiarization/sample_conversation/real/sentence_M.wav'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:32:02.230622Z",
     "start_time": "2024-06-09T11:32:01.150526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "resnet152.register(*speaker1)\n",
    "resnet152.register(*speaker2)"
   ],
   "id": "4d5bb1d046f05f3",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:32:02.236008Z",
     "start_time": "2024-06-09T11:32:02.232626Z"
    }
   },
   "cell_type": "code",
   "source": "user_prompt = f\"Based on the conversations between {speaker1[0]} and {speaker2[0]}, on be half of {speaker2[0]}, do recommend a new topic sentence related the current situation or their personal interests.\"",
   "id": "3b996069095274d4",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test",
   "id": "4794a7f2135b1fbd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:32:02.252077Z",
     "start_time": "2024-06-09T11:32:02.236522Z"
    }
   },
   "cell_type": "code",
   "source": "TEST_MODE = False",
   "id": "bfd4170a9eb3cbec",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:32:02.472567Z",
     "start_time": "2024-06-09T11:32:02.253082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "RECORD_FORMAT = pyaudio.paInt16\n",
    "RECORD_RATE = 44100\n",
    "RECORD_CHANNELS = 1\n",
    "RECORD_CHUNK = 1024\n",
    "recoder = pyaudio.PyAudio()"
   ],
   "id": "6f29cc187768259d",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:32:02.478339Z",
     "start_time": "2024-06-09T11:32:02.473573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "RECORD_SECONDS = 5\n",
    "FRAME_LENGTH = int(RECORD_RATE / RECORD_CHUNK * RECORD_SECONDS)\n",
    "\n",
    "CACHE_FILENAME = \"./cache/cache.wav\"\n",
    "OUTPUT_FILENAME = \"conversation_output.wav\""
   ],
   "id": "a267362d9045abd5",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:32:02.485366Z",
     "start_time": "2024-06-09T11:32:02.479348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from time import sleep\n",
    "\n",
    "def play_test_audio():\n",
    "    audio_path = \"./SpeakerDiarization/sample_conversation/real/conversation_0530_out.wav\"\n",
    "    test_file = wave.open(audio_path, \"rb\")\n",
    "\n",
    "    player = recoder.open(\n",
    "        format=recoder.get_format_from_width(test_file.getsampwidth()),\n",
    "        channels=test_file.getnchannels(),\n",
    "        rate=test_file.getframerate(),\n",
    "        output=True,\n",
    "        stream_callback=lambda _, frame_count, __, ___: (test_file.readframes(frame_count), pyaudio.paContinue)\n",
    "    )\n",
    "\n",
    "    player.start_stream()\n",
    "    print(\"Playing test audio...\")\n",
    "    \n",
    "    while player.is_active():\n",
    "        sleep(0.1)\n",
    "\n",
    "    player.stop_stream()\n",
    "    player.close()"
   ],
   "id": "7ce11f83da97573",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:32:02.493395Z",
     "start_time": "2024-06-09T11:32:02.487375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from multiprocessing import Process\n",
    "\n",
    "RECORD_INTERRUPTED = False\n",
    "RECORDED_FRAMES = []\n",
    "\n",
    "def record_audio():\n",
    "    stream = recoder.open(\n",
    "        format=RECORD_FORMAT, channels=RECORD_CHANNELS,\n",
    "        rate=RECORD_RATE, input=True,\n",
    "        frames_per_buffer=RECORD_CHUNK\n",
    "    )\n",
    "    \n",
    "    print(\"Recording started...\")\n",
    "\n",
    "    if TEST_MODE:\n",
    "        process = Process(target=play_test_audio)\n",
    "        process.start()\n",
    "\n",
    "    output_file = wave.open(OUTPUT_FILENAME, \"wb\")\n",
    "    output_file.setnchannels(RECORD_CHANNELS)\n",
    "    output_file.setsampwidth(recoder.get_sample_size(RECORD_FORMAT))\n",
    "    output_file.setframerate(RECORD_RATE)\n",
    "    \n",
    "    while not RECORD_INTERRUPTED:\n",
    "        frame = b\"\".join([stream.read(RECORD_CHUNK) for _ in range(FRAME_LENGTH)])\n",
    "        output_file.writeframes(frame)\n",
    "        RECORDED_FRAMES.append(frame)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    output_file.close()\n",
    "    if TEST_MODE:\n",
    "        process.terminate()\n",
    "    print(\"Recording stopped.\")"
   ],
   "id": "64218048f65cd5d6",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:35:06.205299Z",
     "start_time": "2024-06-09T11:32:07.668493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from threading import Thread\n",
    "\n",
    "try:\n",
    "    RECORD_INTERRUPTED = False\n",
    "    RECORDED_FRAMES = []\n",
    "    \n",
    "    record_thread = Thread(target=record_audio)\n",
    "    record_thread.start()\n",
    "    \n",
    "    i = 0\n",
    "    last_frame = []\n",
    "    identical = True\n",
    "    \n",
    "    while True:\n",
    "        if not len(RECORDED_FRAMES):\n",
    "            continue\n",
    "        frame = RECORDED_FRAMES.pop(0)\n",
    "        \n",
    "        CACHE = CACHE_FILENAME.replace(\".wav\", f\"_{i}.wav\")\n",
    "        \n",
    "        cache_file = wave.open(CACHE, \"wb\")\n",
    "        cache_file.setnchannels(RECORD_CHANNELS)\n",
    "        cache_file.setsampwidth(recoder.get_sample_size(RECORD_FORMAT))\n",
    "        cache_file.setframerate(RECORD_RATE)\n",
    "        cache_file.writeframes(b\"\".join([*last_frame, frame]))\n",
    "\n",
    "        segments, info = whisper(CACHE, beam_size=5, word_timestamps=False)\n",
    "        print(\"Transcription finished.\")\n",
    "        segments = iter(segments)\n",
    "        \n",
    "        if not identical:\n",
    "            next(segments)\n",
    "            last_frame = [last_frame[-1]]\n",
    "        else:\n",
    "            ChatHistory.messages = ChatHistory.messages[:-1]\n",
    "\n",
    "        for index, segment in enumerate(segments):\n",
    "            try:\n",
    "                embedding = audio.crop(CACHE, Segment(segment.start, segment.end))\n",
    "            except:\n",
    "                embedding = (CACHE, )\n",
    "\n",
    "            print(f\"{i+1}-{index+1}\", (segment.start, segment.end), \"->\", end=\" \", flush=True)\n",
    "\n",
    "            speaker = resnet152(*embedding)\n",
    "            print(\"[%s] %s\" % (speaker['name'], segment.text.strip()))\n",
    "            ChatHistory.add_messages(speaker['name'], segment.text.strip())\n",
    "            \n",
    "            del embedding, speaker\n",
    "            \n",
    "            identical = index <= 1\n",
    "        \n",
    "        if identical:\n",
    "            last_frame = [b\"\".join([*last_frame, frame])]\n",
    "        else:\n",
    "            last_frame.append(frame)\n",
    "\n",
    "        del frame\n",
    "        gc.collect()\n",
    "        \n",
    "        i += 1\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Recording stopped by user\")\n",
    "finally:\n",
    "    RECORD_INTERRUPTED = True\n",
    "    record_thread.join()"
   ],
   "id": "b5046c9735023f32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started...\n",
      "Transcription finished.\n",
      "1-1 (0.0, 5.0) -> [민서] 요즘 가족들이 잘 지내고 계신가요?\n",
      "Transcription finished.\n",
      "2-1 (0.0, 4.4) -> [민서] 요즘 가족들이 잘 지내고 계신가요?\n",
      "2-2 (4.4, 7.36) -> [연우] 네, 가족들은 잘 지내고 있어요.\n",
      "2-3 (7.36, 10.4) -> [민서] 저는 아이들과 스포츠를 즐기는 것 같아요.\n",
      "Transcription finished.\n",
      "Transcription finished.\n",
      "4-1 (2.5, 6.5) -> [민서] 저는 아이들과 스포츠를 즐기는 걸 좋아해요.\n",
      "4-2 (6.5, 9.5) -> [민서] 어떤 종류의 스포츠를 함께 하시나요?\n",
      "4-3 (9.5, 11.9) -> [연우] 축구와 테니스를 함께 하고 있어요.\n",
      "4-4 (11.9, 15.0) -> [민서] 아이들이 적극적으로 참여하면서 즐거운 시간을 보내요.\n",
      "Transcription finished.\n",
      "5-1 (4.48, 11.16) -> [연우] 축구와 테니스를 함께 하고 있어요 아이들이 적극적으로 참여하면서 즐거운 시간을 보내고 있어요\n",
      "5-2 (11.16, 17.04) -> [민서] 축구와 테니스는 정말 가족끼리 함께 하기 좋은 스포츠\n",
      "Transcription finished.\n",
      "6-1 (0.0, 6.0) -> [연우] 축구와 테니스를 함께하고 있어요. 아이들이 적극적으로 참여하면서 즐거운 시간을 보내고 있어요.\n",
      "6-2 (6.0, 15.0) -> [민서] 축구와 테니스는 정말 가족끼리 함께하기 좋은 스포츠죠. 활동적인 시간을 보내면서 가족 간의 유대감도 높일 수 있어요.\n",
      "Transcription finished.\n",
      "7-1 (0.0, 2.0) -> [연우] 축구와 테니스를 함께하고 있어요.\n",
      "7-2 (2.0, 6.0) -> [연우] 아이들이 적극적으로 참여하면서 즐거운 시간을 보내고 있어요.\n",
      "7-3 (6.0, 11.0) -> [민서] 축구와 테니스는 정말 가족끼리 함께하기 좋은 스포츠죠.\n",
      "7-4 (11.0, 17.0) -> [민서] 활동적인 시간을 보내면서 가족 간의 유대감도 높일 수 있을 거예요.\n",
      "7-5 (17.0, 20.0) -> [민서] 다음에는 함께 스프를 즐기며\n",
      "Transcription finished.\n",
      "8-1 (6.2, 16.86) -> [민서] 축구와 테니스는 정말 가족끼리 함께하기 좋은 스포츠죠. 활동적인 시간을 보내면서 가족 간의 유대감도 높일 수 있을 거에요.\n",
      "8-2 (16.86, 23.44) -> [민서] 다음에는 함께 스포츠를 즐기며 가족끼리 더 많은 시간을 보내는 건 어떨까요?\n",
      "8-3 (23.44, 26.240000000000002) -> [민서] 네 좋아요\n",
      "Transcription finished.\n",
      "Recording stopped by user\n",
      "Recording stopped.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:35:09.930245Z",
     "start_time": "2024-06-09T11:35:09.925080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "recoder.terminate()\n",
    "print(\"Recording finished.\")"
   ],
   "id": "9e79e0983cbbc95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording finished.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T11:35:10.623739Z",
     "start_time": "2024-06-09T11:35:10.620344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for message in ChatHistory.messages:\n",
    "    print(f\"[{message['role']}] {message['content']}\")"
   ],
   "id": "94cae5aae65261c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[민서] 요즘 가족들이 잘 지내고 계신가요?\n",
      "[연우] 네, 가족들은 잘 지내고 있어요.\n",
      "[민서] 저는 아이들과 스포츠를 즐기는 것 같아요.\n",
      "[민서] 저는 아이들과 스포츠를 즐기는 걸 좋아해요.\n",
      "[민서] 어떤 종류의 스포츠를 함께 하시나요?\n",
      "[연우] 축구와 테니스를 함께 하고 있어요.\n",
      "[민서] 아이들이 적극적으로 참여하면서 즐거운 시간을 보내요.\n",
      "[연우] 축구와 테니스를 함께 하고 있어요 아이들이 적극적으로 참여하면서 즐거운 시간을 보내고 있어요\n",
      "[연우] 축구와 테니스를 함께하고 있어요. 아이들이 적극적으로 참여하면서 즐거운 시간을 보내고 있어요.\n",
      "[연우] 축구와 테니스를 함께하고 있어요.\n",
      "[연우] 아이들이 적극적으로 참여하면서 즐거운 시간을 보내고 있어요.\n",
      "[민서] 축구와 테니스는 정말 가족끼리 함께하기 좋은 스포츠죠.\n",
      "[민서] 활동적인 시간을 보내면서 가족 간의 유대감도 높일 수 있을 거예요.\n",
      "[민서] 다음에는 함께 스프를 즐기며\n",
      "[민서] 축구와 테니스는 정말 가족끼리 함께하기 좋은 스포츠죠. 활동적인 시간을 보내면서 가족 간의 유대감도 높일 수 있을 거에요.\n",
      "[민서] 다음에는 함께 스포츠를 즐기며 가족끼리 더 많은 시간을 보내는 건 어떨까요?\n",
      "[민서] 네 좋아요\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for chunk in llama3(system_prompt, user_prompt):\n",
    "    print(token_stream(chunk), end=\"\", flush=True)\n",
    "print()"
   ],
   "id": "a3c48d7b0bdbd80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "76fba3854403ce37",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
