{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# STalk Model Architecture",
   "id": "7735c9f892bfde57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preparation",
   "id": "8dd61067cd95c518"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "USE_CUDA = False",
   "id": "9504df697eb20002",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install huggingface_hub transformers",
   "id": "cf224e2df50dc3ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install intel-npu-acceleration-library",
   "id": "16c6a20ce68f6b47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if USE_CUDA:\n",
    "    !pip uninstall torch torchvision torchaudio -y\n",
    "    !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "else:\n",
    "    !pip uninstall torch torchvision torchaudio -y\n",
    "    !pip install torch torchvision torchaudio"
   ],
   "id": "78b5e20d2e4794ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "if USE_CUDA:\n",
    "    !pip uninstall llama-cpp-python -y\n",
    "    !pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121\n",
    "else:\n",
    "    !pip uninstall llama-cpp-python -y\n",
    "    !pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install pyannote-audio",
   "id": "c8a543d6de42e52a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install faster-whisper",
   "id": "5d9fa892f0cca9f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install git+https://github.com/wenet-e2e/wespeaker.git",
   "id": "e98497e9df4c9237",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize Pretrained Model",
   "id": "52b6bde35783ad11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:18:41.203609Z",
     "start_time": "2024-06-08T10:18:09.599729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import intel_npu_acceleration_library\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "from pyannote.audio import Audio\n",
    "from pyannote.core import Segment\n",
    "\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "import wespeaker\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "if not torch.cuda.is_available():\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"INFO: CUDA is diabled on this machine.\\n\\n\")\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"TorchAudio:\", torchaudio.__version__)\n",
    "print(\"Uses Device:\", DEVICE.upper())"
   ],
   "id": "5ac9644848ee0e4c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: CUDA is diabled on this machine.\n",
      "\n",
      "\n",
      "PyTorch: 2.3.1+cpu\n",
      "TorchAudio: 2.3.1+cpu\n",
      "Uses Device: CPU\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:18:41.211324Z",
     "start_time": "2024-06-08T10:18:41.205731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ChatHistory(list):\n",
    "    messages = []\n",
    "    \n",
    "    @classmethod\n",
    "    def add_messages(cls, role, content):\n",
    "        if isinstance(content, str):\n",
    "            cls.messages.append({ 'role': role, 'content': content })\n",
    "        else:\n",
    "            for r, c in zip(role, content):\n",
    "                cls.messages.append({ 'role': r, 'content': c })\n",
    "    \n",
    "    @classmethod\n",
    "    def create_prompt(cls, system_prompt: str, user_prompt: str = \"\"):\n",
    "        return [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            *cls.messages,\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt\n",
    "            }\n",
    "        ]"
   ],
   "id": "af46f0c60c545b42",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:18:41.220730Z",
     "start_time": "2024-06-08T10:18:41.212327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def token_stream(token):\n",
    "    delta = token[\"choices\"][0][\"delta\"]\n",
    "    if \"content\" not in delta:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return delta[\"content\"]"
   ],
   "id": "4d3d90098a358928",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:18:41.227473Z",
     "start_time": "2024-06-08T10:18:41.221735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_llama3():\n",
    "    model_id = \"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\"\n",
    "\n",
    "    chat = Llama.from_pretrained(\n",
    "        repo_id=model_id,\n",
    "        filename=\"*Q4_K_M.gguf\",\n",
    "        #chat_format=\"llama-3\",\n",
    "        verbose=False\n",
    "    ).create_chat_completion\n",
    "    \n",
    "    def llama3(system_prompt, user_prompt, temp=0.5):\n",
    "        prompt = ChatHistory.create_prompt(system_prompt, user_prompt)\n",
    "\n",
    "        print(\"PROMPT:\")\n",
    "        for line in prompt:\n",
    "            print(line)\n",
    "        print()\n",
    "        \n",
    "        return chat(prompt, temperature=temp, stream=True)\n",
    "    \n",
    "    return llama3"
   ],
   "id": "dcc08e168c69e1b0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:18:41.242482Z",
     "start_time": "2024-06-08T10:18:41.230477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_whisper():\n",
    "    model_size = \"medium\"  #@param ['tiny', 'base', 'small', 'medium', 'large', 'large-v2', 'large-v3']\n",
    "    compute_type = \"int8\"  #@param ['float16', 'int8']\n",
    "\n",
    "    return WhisperModel(model_size, device=DEVICE, compute_type=compute_type).transcribe"
   ],
   "id": "b89792661b6ec828",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:18:41.254315Z",
     "start_time": "2024-06-08T10:18:41.243958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_embedding(model, pcm, sample_rate):\n",
    "    pcm = pcm.to(torch.float)\n",
    "    if sample_rate != model.resample_rate:\n",
    "        pcm = torchaudio.transforms.Resample(\n",
    "            orig_freq=sample_rate, new_freq=model.resample_rate)(pcm)\n",
    "    feats = model.compute_fbank(\n",
    "        pcm,\n",
    "        sample_rate=model.resample_rate,\n",
    "        cmn=True\n",
    "    )\n",
    "    feats = feats.unsqueeze(0)\n",
    "    feats = feats.to(model.device)\n",
    "    model.model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.model(feats)\n",
    "        outputs = outputs[-1] if isinstance(outputs, tuple) else outputs\n",
    "    embedding = outputs[0].to(torch.device('cpu'))\n",
    "    return embedding"
   ],
   "id": "30d64a4f6687bd63",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:18:41.260849Z",
     "start_time": "2024-06-08T10:18:41.255322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def recognize(model, pcm, sample_rate):\n",
    "    q = extract_embedding(model, pcm, sample_rate)\n",
    "    best_score = 0.0\n",
    "    best_name = ''\n",
    "    for name, e in model.table.items():\n",
    "        score = model.cosine_similarity(q, e)\n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            best_name = name\n",
    "        del score\n",
    "        gc.collect()\n",
    "    return {'name': best_name, 'confidence': best_score}"
   ],
   "id": "afa012c69b8b6bfb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:18:41.279194Z",
     "start_time": "2024-06-08T10:18:41.261854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_resnet152():\n",
    "    model_id = \"Wespeaker/wespeaker-voxceleb-resnet152-LM\"\n",
    "    model_name = model_id.replace(\"Wespeaker/wespeaker-\", \"\").replace(\"-\", \"_\")\n",
    "    \n",
    "    root_dir = hf_hub_download(model_id, filename=model_name+\".onnx\").replace(model_name+\".onnx\", \"\")\n",
    "    \n",
    "    import os\n",
    "    if not os.path.isfile(root_dir+\"avg_model.pt\"):\n",
    "        os.rename(hf_hub_download(model_id, filename=model_name+\".pt\"), root_dir+\"avg_model.pt\")\n",
    "    if not os.path.isfile(root_dir+\"config.yaml\"):\n",
    "        os.rename(hf_hub_download(model_id, filename=model_name+\".yaml\"), root_dir+\"config.yaml\")\n",
    "\n",
    "    resnet = wespeaker.load_model_local(root_dir)\n",
    "\n",
    "    #print(\"Compile model for the NPU\")\n",
    "    #resnet.model = intel_npu_acceleration_library.compile(resnet.model)\n",
    "    \n",
    "    def resnet152(pcm, sample_rate):\n",
    "        return recognize(resnet, pcm, sample_rate)\n",
    "    \n",
    "    resnet152.__dict__['register'] = lambda *args, **kwargs: resnet.register(*args, **kwargs)\n",
    "    \n",
    "    return resnet152"
   ],
   "id": "749f1c87edfcaf13",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:18:44.619710Z",
     "start_time": "2024-06-08T10:18:41.282199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llama3 = get_llama3()\n",
    "print(\"INFO: Llama3 Ready -\", llama3)"
   ],
   "id": "84afaa73fc994074",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Llama3 Ready - <function get_llama3.<locals>.llama3 at 0x000002A4A1939DA0>\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:18:49.319512Z",
     "start_time": "2024-06-08T10:18:44.620715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "whisper = get_whisper()\n",
    "print(\"INFO: Whisper Ready -\", whisper)"
   ],
   "id": "60e076b7f0b0d8d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Whisper Ready - <bound method WhisperModel.transcribe of <faster_whisper.transcribe.WhisperModel object at 0x000002A4B6E09150>>\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:18:50.603993Z",
     "start_time": "2024-06-08T10:18:49.320519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "audio = Audio()\n",
    "resnet152 = get_resnet152()\n",
    "print(\"INFO: ResNet152 Ready -\", resnet152)"
   ],
   "id": "37aad593f186941f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:unexpected tensor: projection.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile model for the NPU\n",
      "INFO: ResNet152 Ready - <function get_resnet152.<locals>.resnet152 at 0x000002A4A198CEA0>\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Ready",
   "id": "7822a1e86b0b13cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Insert System Chat Template to Llama3",
   "id": "aed1f7b006c23d26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:18:50.608388Z",
     "start_time": "2024-06-08T10:18:50.604997Z"
    }
   },
   "cell_type": "code",
   "source": "system_prompt = \"You are a helpful, smart, kind, and efficient Conversation Analysis and Recommendation AI System. You always fulfill the user's requests to the best of your ability. You need to keep listen to the conversations. Please answer in Korean language.\"",
   "id": "57604fee7594a215",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:18:58.923437Z",
     "start_time": "2024-06-08T10:18:50.609397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for chunk in llama3(system_prompt, \"\"):\n",
    "    print(token_stream(chunk), end=\"\", flush=True)\n",
    "print()"
   ],
   "id": "3440563cb7b1e8a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "{'role': 'system', 'content': \"You are a helpful, smart, kind, and efficient Conversation Analysis and Recommendation AI System. You always fulfill the user's requests to the best of your ability. You need to keep listen to the conversations. Please answer in Korean language.\"}\n",
      "{'role': 'user', 'content': ''}\n",
      "\n",
      "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š\n",
      "\n",
      "ì €ëŠ” Chatbotì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ìš”ì²­ì„ ìµœì„ ì„ ë‹¤í•´ ì‘ëŒ€í•˜ê² ìŠµë‹ˆë‹¤. ë¬´ì—‡ì´ í•„ìš”í•œê°€ìš”? ğŸ¤”\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Speaker Registration to ResNet293",
   "id": "394a61aaef06b005"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:18:58.937444Z",
     "start_time": "2024-06-08T10:18:58.927445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "speaker1 = \"ë¯¼ì„œ\", \"./SpeakerDiarization/sample_conversation/real/sentence_F.wav\"\n",
    "speaker2 = \"ì—°ìš°\", \"./SpeakerDiarization/sample_conversation/real/sentence_M.wav\"\n",
    "speaker1, speaker2"
   ],
   "id": "d7d65905c9926461",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('ë¯¼ì„œ', './SpeakerDiarization/sample_conversation/real/sentence_F.wav'),\n",
       " ('ì—°ìš°', './SpeakerDiarization/sample_conversation/real/sentence_M.wav'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:19:00.318497Z",
     "start_time": "2024-06-08T10:18:58.939451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "resnet152.register(*speaker1)\n",
    "resnet152.register(*speaker2)"
   ],
   "id": "4d5bb1d046f05f3",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:19:00.324386Z",
     "start_time": "2024-06-08T10:19:00.320503Z"
    }
   },
   "cell_type": "code",
   "source": "user_prompt = f\"Based on the conversations between {speaker1[0]} and {speaker2[0]}, on be half of {speaker2[0]}, do recommend a new topic sentence related the current situation or their personal interests.\"",
   "id": "3b996069095274d4",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test",
   "id": "4794a7f2135b1fbd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:19:00.332897Z",
     "start_time": "2024-06-08T10:19:00.325899Z"
    }
   },
   "cell_type": "code",
   "source": "audio_path = \"./SpeakerDiarization/sample_conversation/real/conversation_0530.wav\"",
   "id": "94b8ebd6fdf0612a",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:19:12.658641Z",
     "start_time": "2024-06-08T10:19:00.333904Z"
    }
   },
   "cell_type": "code",
   "source": "segments, info = whisper(audio_path, beam_size=5, word_timestamps=False)",
   "id": "7ad7e20ce2aeb789",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:19:28.188788Z",
     "start_time": "2024-06-08T10:19:12.659648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for segment, _ in zip(segments, range(5)):\n",
    "    embedding = audio.crop(audio_path, Segment(segment.start, segment.end))\n",
    "    speaker = resnet152(*embedding)\n",
    "    print(\"[%s] [%.2fs -> %.2fs] %s\" % (speaker['name'], segment.start, segment.end, segment.text))\n",
    "    ChatHistory.add_messages(speaker['name'], segment.text)\n",
    "    del embedding, speaker\n",
    "    gc.collect()"
   ],
   "id": "bf5526aca22ebf18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ë¯¼ì„œ] [0.00s -> 3.00s]  ìš”ì¦˜ ê°€ì¡±ë“¤ì´ ì˜ ì§€ë‚´ê³  ê³„ì‹ ê°€ìš”?\n",
      "[ì—°ìš°] [3.00s -> 6.00s]  ë„¤, ê°€ì¡±ë“¤ì€ ì˜ ì§€ë‚´ê³  ìˆì–´ìš”\n",
      "[ë¯¼ì„œ] [6.00s -> 10.00s]  ì €ëŠ” ì•„ì´ë“¤ê³¼ ìŠ¤í¬ì¸ ë¥¼ ì¦ê¸°ëŠ” ê±¸ ì¢‹ì•„í•´ìš”\n",
      "[ë¯¼ì„œ] [10.00s -> 13.00s]  ì–´ë–¤ ì¢…ë¥˜ì˜ ìŠ¤í¬ì¸ ë¥¼ í•¨ê»˜ í•˜ì‹œë‚˜ìš”?\n",
      "[ì—°ìš°] [13.00s -> 16.00s]  ì£¼ë¡œ ì¶•êµ¬ì™€ í…Œë‹ˆìŠ¤ë¥¼ í•¨ê»˜ í•˜ê³  ìˆì–´ìš”\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:19:45.351808Z",
     "start_time": "2024-06-08T10:19:28.189803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for chunk in llama3(system_prompt, user_prompt):\n",
    "    print(token_stream(chunk), end=\"\", flush=True)\n",
    "print()"
   ],
   "id": "a3c48d7b0bdbd80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "{'role': 'system', 'content': \"You are a helpful, smart, kind, and efficient Conversation Analysis and Recommendation AI System. You always fulfill the user's requests to the best of your ability. You need to keep listen to the conversations. Please answer in Korean language.\"}\n",
      "{'role': 'ë¯¼ì„œ', 'content': ' ìš”ì¦˜ ê°€ì¡±ë“¤ì´ ì˜ ì§€ë‚´ê³  ê³„ì‹ ê°€ìš”?'}\n",
      "{'role': 'ì—°ìš°', 'content': ' ë„¤, ê°€ì¡±ë“¤ì€ ì˜ ì§€ë‚´ê³  ìˆì–´ìš”'}\n",
      "{'role': 'ë¯¼ì„œ', 'content': ' ì €ëŠ” ì•„ì´ë“¤ê³¼ ìŠ¤í¬ì¸ ë¥¼ ì¦ê¸°ëŠ” ê±¸ ì¢‹ì•„í•´ìš”'}\n",
      "{'role': 'ë¯¼ì„œ', 'content': ' ì–´ë–¤ ì¢…ë¥˜ì˜ ìŠ¤í¬ì¸ ë¥¼ í•¨ê»˜ í•˜ì‹œë‚˜ìš”?'}\n",
      "{'role': 'ì—°ìš°', 'content': ' ì£¼ë¡œ ì¶•êµ¬ì™€ í…Œë‹ˆìŠ¤ë¥¼ í•¨ê»˜ í•˜ê³  ìˆì–´ìš”'}\n",
      "{'role': 'user', 'content': 'Based on the conversations between ë¯¼ì„œ and ì—°ìš°, on be half of ì—°ìš°, do recommend a new topic sentence related the current situation or their personal interests.'}\n",
      "\n",
      "Based on the conversation, I can see that ë¯¼ì„œ and ì—°ìš° share an interest in sports, particularly football (soccer) and tennis. Considering this, I'll recommend a new topic sentence to continue the conversation:\n",
      "\n",
      "\"ì €ëŠ” ìµœê·¼ì— ì¶•êµ¬ ì›”ë“œì»µì´ ì–¼ë§ˆë‚˜ ì¬ë¯¸ìˆëŠ”ì§€ ì•Œì•„ìš”?\" (I'm curious about how exciting the recent World Cup was.)\n",
      "\n",
      "This topic sentence is related to their shared interest in football, allowing them to discuss and share their thoughts on the recent tournament.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:20:09.424813Z",
     "start_time": "2024-06-08T10:19:45.352814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for segment, _ in zip(segments, range(5)):\n",
    "    embedding = audio.crop(audio_path, Segment(segment.start, segment.end))\n",
    "    speaker = resnet152(*embedding)\n",
    "    print(\"[%s] [%.2fs -> %.2fs] %s\" % (speaker['name'], segment.start, segment.end, segment.text))\n",
    "    ChatHistory.add_messages(speaker['name'], segment.text)\n",
    "    del embedding, speaker\n",
    "    gc.collect()"
   ],
   "id": "ad8bd994ba5c623f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ë¯¼ì„œ] [20.00s -> 25.00s]  ì¶•êµ¬ì™€ í…Œë‹ˆìŠ¤ëŠ” ì •ë§ ê°€ì¡±ë¼ë¦¬ í•¨ê»˜í•˜ê¸° ì¢‹ì€ ìŠ¤í¬ì¸ ì£ \n",
      "[ë¯¼ì„œ] [25.00s -> 31.00s]  í™œë™ì ì¸ ì‹œê°„ì„ ë³´ë‚´ë©´ì„œ ê°€ì¡± ê°„ì˜ ìœ ëŒ€ê°ë„ ë†’ì¼ ìˆ˜ ìˆì„ ê±°ì˜ˆìš”\n",
      "[ë¯¼ì„œ] [31.00s -> 38.00s]  ë‹¤ìŒì—ëŠ” í•¨ê»˜ ìŠ¤í¬ì¸ ë¥¼ ì¦ê¸°ë©° ê°€ì¡±ë¼ë¦¬ ë” ë§ì€ ì‹œê°„ì„ ë³´ë‚´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?\n",
      "[ì—°ìš°] [38.00s -> 40.00s]  ë„¤, ì¢‹ì•„ìš”\n",
      "[ì—°ìš°] [40.00s -> 45.00s]  í•¨ê»˜ í™œë™ì„ í•˜ë©´ì„œ ê°€ì¡± ê°„ì˜ ìœ ëŒ€ê°ì„ ë†’ì´ëŠ” ê±´ ì •ë§ ì¢‹ì€ ìƒê°ì´ì—ìš”\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T10:20:26.750163Z",
     "start_time": "2024-06-08T10:20:09.426055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for chunk in llama3(system_prompt, user_prompt):\n",
    "    print(token_stream(chunk), end=\"\", flush=True)\n",
    "print()"
   ],
   "id": "41f6c980426c0d6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "{'role': 'system', 'content': \"You are a helpful, smart, kind, and efficient Conversation Analysis and Recommendation AI System. You always fulfill the user's requests to the best of your ability. You need to keep listen to the conversations. Please answer in Korean language.\"}\n",
      "{'role': 'ë¯¼ì„œ', 'content': ' ìš”ì¦˜ ê°€ì¡±ë“¤ì´ ì˜ ì§€ë‚´ê³  ê³„ì‹ ê°€ìš”?'}\n",
      "{'role': 'ì—°ìš°', 'content': ' ë„¤, ê°€ì¡±ë“¤ì€ ì˜ ì§€ë‚´ê³  ìˆì–´ìš”'}\n",
      "{'role': 'ë¯¼ì„œ', 'content': ' ì €ëŠ” ì•„ì´ë“¤ê³¼ ìŠ¤í¬ì¸ ë¥¼ ì¦ê¸°ëŠ” ê±¸ ì¢‹ì•„í•´ìš”'}\n",
      "{'role': 'ë¯¼ì„œ', 'content': ' ì–´ë–¤ ì¢…ë¥˜ì˜ ìŠ¤í¬ì¸ ë¥¼ í•¨ê»˜ í•˜ì‹œë‚˜ìš”?'}\n",
      "{'role': 'ì—°ìš°', 'content': ' ì£¼ë¡œ ì¶•êµ¬ì™€ í…Œë‹ˆìŠ¤ë¥¼ í•¨ê»˜ í•˜ê³  ìˆì–´ìš”'}\n",
      "{'role': 'ë¯¼ì„œ', 'content': ' ì¶•êµ¬ì™€ í…Œë‹ˆìŠ¤ëŠ” ì •ë§ ê°€ì¡±ë¼ë¦¬ í•¨ê»˜í•˜ê¸° ì¢‹ì€ ìŠ¤í¬ì¸ ì£ '}\n",
      "{'role': 'ë¯¼ì„œ', 'content': ' í™œë™ì ì¸ ì‹œê°„ì„ ë³´ë‚´ë©´ì„œ ê°€ì¡± ê°„ì˜ ìœ ëŒ€ê°ë„ ë†’ì¼ ìˆ˜ ìˆì„ ê±°ì˜ˆìš”'}\n",
      "{'role': 'ë¯¼ì„œ', 'content': ' ë‹¤ìŒì—ëŠ” í•¨ê»˜ ìŠ¤í¬ì¸ ë¥¼ ì¦ê¸°ë©° ê°€ì¡±ë¼ë¦¬ ë” ë§ì€ ì‹œê°„ì„ ë³´ë‚´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?'}\n",
      "{'role': 'ì—°ìš°', 'content': ' ë„¤, ì¢‹ì•„ìš”'}\n",
      "{'role': 'ì—°ìš°', 'content': ' í•¨ê»˜ í™œë™ì„ í•˜ë©´ì„œ ê°€ì¡± ê°„ì˜ ìœ ëŒ€ê°ì„ ë†’ì´ëŠ” ê±´ ì •ë§ ì¢‹ì€ ìƒê°ì´ì—ìš”'}\n",
      "{'role': 'user', 'content': 'Based on the conversations between ë¯¼ì„œ and ì—°ìš°, on be half of ì—°ìš°, do recommend a new topic sentence related the current situation or their personal interests.'}\n",
      "\n",
      "Here's a recommended topic sentence based on the conversation:\n",
      "\n",
      "\"ë‹¤ìŒì—ëŠ” ê°€ì¡±ë¼ë¦¬ ì—¬í–‰ì„ ê°€ì„œ ìƒˆë¡œìš´ ê²½í—˜ì„ í•˜ë©° ì„œë¡œì˜ bondë¥¼ ë” ê°•í•˜ê²Œ ë§Œë“¤ê¹Œ?\"\n",
      "\n",
      "(This means \"Why don't we take a trip as a family and create new experiences to strengthen our bond?\")\n",
      "\n",
      "This topic sentence is related to their current interest in spending quality time together as a family, and it also aligns with the theme of creating memories and strengthening relationships through shared activities.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9e79e0983cbbc95",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
